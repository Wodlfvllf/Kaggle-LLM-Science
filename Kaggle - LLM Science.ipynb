{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-12T02:30:11.628738Z","iopub.execute_input":"2023-09-12T02:30:11.629381Z","iopub.status.idle":"2023-09-12T02:30:11.637239Z","shell.execute_reply.started":"2023-09-12T02:30:11.629345Z","shell.execute_reply":"2023-09-12T02:30:11.636260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n!pip install -U /kaggle/working/sentence-transformers\n!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/d/shashankshuklacodedl/llm-whls/llm\\ whls/peft-0.5.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:30:11.639256Z","iopub.execute_input":"2023-09-12T02:30:11.639874Z","iopub.status.idle":"2023-09-12T02:31:17.909556Z","shell.execute_reply.started":"2023-09-12T02:30:11.639841Z","shell.execute_reply":"2023-09-12T02:31:17.908328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport re\nfrom tqdm.auto import tqdm\nimport blingfire as bf\nfrom __future__ import annotations\n\nfrom collections.abc import Iterable\n\nimport faiss\nfrom faiss import write_index, read_index\n\nfrom sentence_transformers import SentenceTransformer\n\nimport torch\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\n\nfrom dataclasses import dataclass\nfrom typing import Optional, Union\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom torch.utils.data import DataLoader","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:31:17.911286Z","iopub.execute_input":"2023-09-12T02:31:17.912227Z","iopub.status.idle":"2023-09-12T02:31:33.200463Z","shell.execute_reply.started":"2023-09-12T02:31:17.912190Z","shell.execute_reply":"2023-09-12T02:31:33.199511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_documents(documents: Iterable[str],\n                      document_ids: Iterable,\n                      split_sentences: bool = True,\n                      filter_len: int = 3,\n                      disable_progress_bar: bool = False) -> pd.DataFrame:\n    \n    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n\n    if split_sentences:\n        df = sentencize(df.text.values, \n                        df.document_id.values,\n                        df.offset.values, \n                        filter_len, \n                        disable_progress_bar)\n    return df\n\ndef sectionize_documents(documents: Iterable[str],\n                         document_ids: Iterable,\n                         disable_progress_bar: bool = False) -> pd.DataFrame:\n    \n    processed_documents = []\n    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n        row = {}\n        text, start, end = (document, 0, len(document))\n        row['document_id'] = document_id\n        row['text'] = text\n        row['offset'] = (start, end)\n\n        processed_documents.append(row)\n\n    _df = pd.DataFrame(processed_documents)\n    if _df.shape[0] > 0:\n        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n    else:\n        return _df\n    \ndef sentencize(documents: Iterable[str],\n               document_ids: Iterable,\n               offsets: Iterable[tuple[int, int]],\n               filter_len: int = 3,\n               disable_progress_bar: bool = False) -> pd.DataFrame:\n    \n    document_sentences = []\n    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents), disable=disable_progress_bar):\n        try:\n            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n            for o in sentence_offsets:\n                if o[1]-o[0] > filter_len:\n                    sentence = document[o[0]:o[1]]\n                    abs_offsets = (o[0]+offset[0], o[1]+offset[0])\n                    row = {}\n                    row['document_id'] = document_id\n                    row['text'] = sentence\n                    row['offset'] = abs_offsets\n                    document_sentences.append(row)\n        except:\n            continue\n    return pd.DataFrame(document_sentences)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:31:33.202792Z","iopub.execute_input":"2023-09-12T02:31:33.203077Z","iopub.status.idle":"2023-09-12T02:31:33.217741Z","shell.execute_reply.started":"2023-09-12T02:31:33.203052Z","shell.execute_reply":"2023-09-12T02:31:33.216686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SIM_MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\nDEVICE = 0\nMAX_LENGTH = 384\nBATCH_SIZE = 16\n\nWIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\nwiki_files = os.listdir(WIKI_PATH)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:31:33.219045Z","iopub.execute_input":"2023-09-12T02:31:33.219797Z","iopub.status.idle":"2023-09-12T02:31:33.239054Z","shell.execute_reply.started":"2023-09-12T02:31:33.219765Z","shell.execute_reply":"2023-09-12T02:31:33.238115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\ntrn.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:31:33.240759Z","iopub.execute_input":"2023-09-12T02:31:33.241134Z","iopub.status.idle":"2023-09-12T02:31:33.276832Z","shell.execute_reply.started":"2023-09-12T02:31:33.241102Z","shell.execute_reply":"2023-09-12T02:31:33.275846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn = trn.drop('id', axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:31:33.278528Z","iopub.execute_input":"2023-09-12T02:31:33.278965Z","iopub.status.idle":"2023-09-12T02:31:33.290691Z","shell.execute_reply.started":"2023-09-12T02:31:33.278917Z","shell.execute_reply":"2023-09-12T02:31:33.289555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:31:33.292221Z","iopub.execute_input":"2023-09-12T02:31:33.293323Z","iopub.status.idle":"2023-09-12T02:31:33.309370Z","shell.execute_reply.started":"2023-09-12T02:31:33.293248Z","shell.execute_reply":"2023-09-12T02:31:33.308107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SentenceTransformer(SIM_MODEL, device='cuda')\nmodel.max_seq_length = MAX_LENGTH\nmodel = model.half()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:31:33.314821Z","iopub.execute_input":"2023-09-12T02:31:33.315159Z","iopub.status.idle":"2023-09-12T02:31:35.454385Z","shell.execute_reply.started":"2023-09-12T02:31:33.315131Z","shell.execute_reply":"2023-09-12T02:31:35.453248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:31:35.455895Z","iopub.execute_input":"2023-09-12T02:31:35.456314Z","iopub.status.idle":"2023-09-12T02:33:04.112343Z","shell.execute_reply.started":"2023-09-12T02:31:35.456276Z","shell.execute_reply":"2023-09-12T02:33:04.110154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt_embeddings = model.encode(trn.prompt.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nprompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:33:04.114925Z","iopub.execute_input":"2023-09-12T02:33:04.115460Z","iopub.status.idle":"2023-09-12T02:33:13.557213Z","shell.execute_reply.started":"2023-09-12T02:33:04.115424Z","shell.execute_reply":"2023-09-12T02:33:13.556214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search_score, search_index = sentence_index.search(prompt_embeddings, 3)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:33:13.558561Z","iopub.execute_input":"2023-09-12T02:33:13.558991Z","iopub.status.idle":"2023-09-12T02:33:36.585082Z","shell.execute_reply.started":"2023-09-12T02:33:13.558957Z","shell.execute_reply":"2023-09-12T02:33:36.584229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del sentence_index\ndel prompt_embeddings\n_ = gc.collect()\nlibc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:33:36.586656Z","iopub.execute_input":"2023-09-12T02:33:36.587378Z","iopub.status.idle":"2023-09-12T02:33:37.405006Z","shell.execute_reply.started":"2023-09-12T02:33:36.587342Z","shell.execute_reply":"2023-09-12T02:33:37.404028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\",\n                     columns=['id', 'file'])","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:33:37.406781Z","iopub.execute_input":"2023-09-12T02:33:37.407489Z","iopub.status.idle":"2023-09-12T02:33:41.722266Z","shell.execute_reply.started":"2023-09-12T02:33:37.407453Z","shell.execute_reply":"2023-09-12T02:33:41.721228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Get the article and associated file location using the index\nwikipedia_file_data = []\n\nfor i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n    scr_idx = idx\n    _df = df.loc[scr_idx].copy()\n    _df['prompt_id'] = i\n    wikipedia_file_data.append(_df)\nwikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\nwikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n\n## Save memory - delete df since it is no longer necessary\ndel df\n_ = gc.collect()\nlibc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:33:41.723449Z","iopub.execute_input":"2023-09-12T02:33:41.723744Z","iopub.status.idle":"2023-09-12T02:33:42.427078Z","shell.execute_reply.started":"2023-09-12T02:33:41.723718Z","shell.execute_reply":"2023-09-12T02:33:42.426206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Get the full text data\nwiki_text_data = []\n\nfor file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n    _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]\n    _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text'])\n\n    _df_temp = _df[_df['id'].isin(_id)].copy()\n    del _df\n    _ = gc.collect()\n    libc.malloc_trim(0)\n    wiki_text_data.append(_df_temp)\nwiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:33:42.428439Z","iopub.execute_input":"2023-09-12T02:33:42.428770Z","iopub.status.idle":"2023-09-12T02:38:14.430107Z","shell.execute_reply.started":"2023-09-12T02:33:42.428745Z","shell.execute_reply":"2023-09-12T02:38:14.429119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_wiki_text_data = process_documents(wiki_text_data.text.values, wiki_text_data.id.values)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:38:14.431715Z","iopub.execute_input":"2023-09-12T02:38:14.432097Z","iopub.status.idle":"2023-09-12T02:38:19.359861Z","shell.execute_reply.started":"2023-09-12T02:38:14.432064Z","shell.execute_reply":"2023-09-12T02:38:19.358906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Get embeddings of the wiki text data\nwiki_data_embeddings = model.encode(processed_wiki_text_data.text,\n                                    batch_size=BATCH_SIZE,\n                                    device=DEVICE,\n                                    show_progress_bar=True,\n                                    convert_to_tensor=True,\n                                    normalize_embeddings=True)#.half()\nwiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:38:19.361425Z","iopub.execute_input":"2023-09-12T02:38:19.361784Z","iopub.status.idle":"2023-09-12T02:38:42.796579Z","shell.execute_reply.started":"2023-09-12T02:38:19.361744Z","shell.execute_reply":"2023-09-12T02:38:42.795538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:38:42.798524Z","iopub.execute_input":"2023-09-12T02:38:42.798915Z","iopub.status.idle":"2023-09-12T02:38:43.074830Z","shell.execute_reply.started":"2023-09-12T02:38:42.798877Z","shell.execute_reply":"2023-09-12T02:38:43.073252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Combine all answers\ntrn['answer_all'] = trn.apply(lambda x: \" \".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n\n\n## Search using the prompt and answers to guide the search\ntrn['prompt_answer_stem'] = trn['prompt'] + \" \" + trn['answer_all']","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:38:43.076775Z","iopub.execute_input":"2023-09-12T02:38:43.077208Z","iopub.status.idle":"2023-09-12T02:38:43.095290Z","shell.execute_reply.started":"2023-09-12T02:38:43.077146Z","shell.execute_reply":"2023-09-12T02:38:43.094271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nquestion_embeddings = question_embeddings.detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:38:43.096683Z","iopub.execute_input":"2023-09-12T02:38:43.097276Z","iopub.status.idle":"2023-09-12T02:38:43.495899Z","shell.execute_reply.started":"2023-09-12T02:38:43.097239Z","shell.execute_reply":"2023-09-12T02:38:43.494829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Parameter to determine how many relevant sentences to include\nNUM_SENTENCES_INCLUDE = 10\n\n## List containing just Context\ncontexts = []\n\nfor r in tqdm(trn.itertuples(), total=len(trn)):\n\n    prompt_id = r.Index\n\n    prompt_indices = processed_wiki_text_data[processed_wiki_text_data['document_id'].isin(wikipedia_file_data[wikipedia_file_data['prompt_id']==prompt_id]['id'].values)].index.values\n\n    if prompt_indices.shape[0] > 0:\n        prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n        prompt_index.add(wiki_data_embeddings[prompt_indices])\n\n        context = \"\"\n        \n        ## Get the top matches\n        ss, ii = prompt_index.search(question_embeddings, NUM_SENTENCES_INCLUDE)\n        for _s, _i in zip(ss[prompt_id], ii[prompt_id]):\n            context += processed_wiki_text_data.loc[prompt_indices]['text'].iloc[_i] + \" \"\n        \n    contexts.append(context)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:38:43.497539Z","iopub.execute_input":"2023-09-12T02:38:43.498121Z","iopub.status.idle":"2023-09-12T02:38:45.572113Z","shell.execute_reply.started":"2023-09-12T02:38:43.498086Z","shell.execute_reply":"2023-09-12T02:38:45.571065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn['context'] = contexts\ntrn[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\"]].to_csv(\"./test_context.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:38:45.573774Z","iopub.execute_input":"2023-09-12T02:38:45.574348Z","iopub.status.idle":"2023-09-12T02:38:45.620450Z","shell.execute_reply.started":"2023-09-12T02:38:45.574315Z","shell.execute_reply":"2023-09-12T02:38:45.619487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"test_context.csv\")\ntest_df.index = list(range(len(test_df)))\ntest_df['id'] = list(range(len(test_df)))\ntest_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:1750]) + \" #### \" +  test_df[\"prompt\"]\ntest_df['answer'] = 'A'","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:38:45.626576Z","iopub.execute_input":"2023-09-12T02:38:45.626876Z","iopub.status.idle":"2023-09-12T02:38:45.653370Z","shell.execute_reply.started":"2023-09-12T02:38:45.626848Z","shell.execute_reply":"2023-09-12T02:38:45.652319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dir = \"/kaggle/input/llm-science-run-context-2\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:38:45.654912Z","iopub.execute_input":"2023-09-12T02:38:45.655326Z","iopub.status.idle":"2023-09-12T02:39:04.964403Z","shell.execute_reply.started":"2023-09-12T02:38:45.655289Z","shell.execute_reply":"2023-09-12T02:39:04.963242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"options = 'ABCDE'\nindices = list(range(5))\n\noption_to_index = {option: index for option, index in zip(options, indices)}\nindex_to_option = {index: option for option, index in zip(options, indices)}\n\ndef preprocess(example):\n    # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n    # so we'll copy our question 5 times before tokenizing\n    first_sentence = [example['prompt']] * 5\n    second_sentence = []\n    for option in options:\n        second_sentence.append(example[option])\n    # Our tokenizer will turn our text into token IDs BERT can understand\n    tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n    tokenized_example['label'] = option_to_index[example['answer']]\n    return tokenized_example","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:39:04.966191Z","iopub.execute_input":"2023-09-12T02:39:04.966558Z","iopub.status.idle":"2023-09-12T02:39:04.974072Z","shell.execute_reply.started":"2023-09-12T02:39:04.966522Z","shell.execute_reply":"2023-09-12T02:39:04.972964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    \n    def __call__(self, features):\n        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0]['input_ids'])\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])\n        \n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors='pt',\n        )\n        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n        return batch","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:39:04.975598Z","iopub.execute_input":"2023-09-12T02:39:04.976028Z","iopub.status.idle":"2023-09-12T02:39:04.990037Z","shell.execute_reply.started":"2023-09-12T02:39:04.975993Z","shell.execute_reply":"2023-09-12T02:39:04.988938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_test_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\ntokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\ndata_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\ntest_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:39:04.991353Z","iopub.execute_input":"2023-09-12T02:39:04.991834Z","iopub.status.idle":"2023-09-12T02:39:06.415397Z","shell.execute_reply.started":"2023-09-12T02:39:04.991802Z","shell.execute_reply":"2023-09-12T02:39:06.414473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictions.append(outputs.logits.cpu().detach())\n\ntest_predictions = torch.cat(test_predictions)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:39:06.416737Z","iopub.execute_input":"2023-09-12T02:39:06.417192Z","iopub.status.idle":"2023-09-12T02:41:12.593326Z","shell.execute_reply.started":"2023-09-12T02:39:06.417142Z","shell.execute_reply":"2023-09-12T02:41:12.592228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_preds_my_runs_1 = []\nall_preds_my_runs_2 = []\nall_preds_my_runs_3 = []\nfor i in range(3):\n    model_l = AutoModelForMultipleChoice.from_pretrained(f'/kaggle/input/science-exam-trained-model-weights/run_{i}').cuda()\n    model_l.eval()\n    preds = []\n    for batch in test_dataloader:\n        for k in batch.keys():\n            batch[k] = batch[k].cuda()\n        with torch.no_grad():\n            outputs = model_l(**batch)\n        preds.append(outputs.logits.cpu().detach())\n    del model_l\n    exec(f'all_preds_my_runs_{i} = torch.cat(preds)')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:41:12.594852Z","iopub.execute_input":"2023-09-12T02:41:12.595826Z","iopub.status.idle":"2023-09-12T02:48:32.474653Z","shell.execute_reply.started":"2023-09-12T02:41:12.595789Z","shell.execute_reply":"2023-09-12T02:48:32.473616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForMultipleChoice.from_pretrained(f'/kaggle/input/deberta-v3-large-hf-weights').cuda()\nmodel.eval()\npreds = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    preds.append(outputs.logits.cpu().detach())\n\nhyc_preds = torch.cat(preds)\ndel model\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:48:32.476362Z","iopub.execute_input":"2023-09-12T02:48:32.476727Z","iopub.status.idle":"2023-09-12T02:50:47.312949Z","shell.execute_reply.started":"2023-09-12T02:48:32.476692Z","shell.execute_reply":"2023-09-12T02:50:47.311915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel = AutoModelForMultipleChoice.from_pretrained(f'/kaggle/input/llm-se-debertav3-large').cuda()\nmodel.eval()\npreds = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    preds.append(outputs.logits.cpu().detach())\n\nl_preds = torch.cat(preds)\ndel model\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:50:47.314762Z","iopub.execute_input":"2023-09-12T02:50:47.315149Z","iopub.status.idle":"2023-09-12T02:53:13.848980Z","shell.execute_reply.started":"2023-09-12T02:50:47.315114Z","shell.execute_reply":"2023-09-12T02:53:13.847915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predictions_to_map_output(predictions):\n    sorted_answer_indices = np.argsort(-predictions)\n    top_answer_indices = sorted_answer_indices[:,:3] # Get the first three answers in each row\n    top_answers = np.vectorize(index_to_option.get)(top_answer_indices)\n    return np.apply_along_axis(lambda row: ' '.join(row), 1, top_answers)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:53:13.850620Z","iopub.execute_input":"2023-09-12T02:53:13.851408Z","iopub.status.idle":"2023-09-12T02:53:13.858753Z","shell.execute_reply.started":"2023-09-12T02:53:13.851362Z","shell.execute_reply":"2023-09-12T02:53:13.857555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all_preds_my_runs_3\ndef softmax(x):\n    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return e_x / e_x.sum(axis=-1, keepdims = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:53:13.860309Z","iopub.execute_input":"2023-09-12T02:53:13.860906Z","iopub.status.idle":"2023-09-12T02:53:13.870014Z","shell.execute_reply.started":"2023-09-12T02:53:13.860870Z","shell.execute_reply":"2023-09-12T02:53:13.869048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deberta_large = softmax(all_preds_my_runs_1.numpy())+softmax(all_preds_my_runs_2.numpy())\nall_pred = softmax(l_preds.numpy())+softmax(hyc_preds.numpy())+ deberta_large\nprediction = softmax(all_pred) + softmax(test_predictions.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:53:13.871492Z","iopub.execute_input":"2023-09-12T02:53:13.871871Z","iopub.status.idle":"2023-09-12T02:53:13.886774Z","shell.execute_reply.started":"2023-09-12T02:53:13.871839Z","shell.execute_reply":"2023-09-12T02:53:13.885684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# deberta_large = 0.5*all_preds_my_runs_1+0.5*all_preds_my_runs_2\n# all_pred = 0.7*(0.4*l_preds + 0.6*hyc_preds) + 0.3*deberta_large\n# prediction = 0.6*all_pred + 0.5*test_predictions ","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:53:13.888502Z","iopub.execute_input":"2023-09-12T02:53:13.888882Z","iopub.status.idle":"2023-09-12T02:53:13.893758Z","shell.execute_reply.started":"2023-09-12T02:53:13.888846Z","shell.execute_reply":"2023-09-12T02:53:13.892642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def map_at_3(predictions, labels):\n    map_sum = 0\n    pred = np.argsort(-1*np.array(predictions),axis=1)[:,:3]\n    for x,y in zip(pred,labels):\n        z = [1/i if y==j else 0 for i,j in zip([1,2,3],x)]\n        map_sum += np.sum(z)\n    return map_sum / len(predictions)\n\ndef compute_metrics(p):\n    predictions = p.predictions.tolist()\n    labels = p.label_ids.tolist()\n    return {\"map@3\": map_at_3(predictions, labels)}","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:53:13.895982Z","iopub.execute_input":"2023-09-12T02:53:13.896394Z","iopub.status.idle":"2023-09-12T02:53:13.906092Z","shell.execute_reply.started":"2023-09-12T02:53:13.896361Z","shell.execute_reply":"2023-09-12T02:53:13.904854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame({\"id\": np.arange(len(test_df))})\nsubmission_df['prediction'] = predictions_to_map_output(prediction)\n\nsubmission_df.head()\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T02:53:13.907623Z","iopub.execute_input":"2023-09-12T02:53:13.908112Z","iopub.status.idle":"2023-09-12T02:53:13.919871Z","shell.execute_reply.started":"2023-09-12T02:53:13.908076Z","shell.execute_reply":"2023-09-12T02:53:13.918884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}